# Stage 3 - Асинхронный сервер
* Теперь задачи работы с сокетом и задачи работы с хранилищем разделены
* Запросы к /status работают быстрее и не ждут пока выполнятся другие запросы, связанные с хранилищем

## GET
### CPU
Селекторы теперь только разгребают запросы и ставят задачи в очередь, а работу с хранилищем и ответом занимаются воркеры
### ALLOC
Больше всего памяти выделяется воркерами при запросах к хранилищу
### LOCK
Методы `sendResponse` и `sendError` у `HttpSession` - synchronized, на них и локи

## PUT
### CPU
На работу с сокетом времени тратится больше, чем непосредственно на саму вставку
### ALLOC
Аналогично GET, большая часть памяти выделяется на вставку в хранилище
### LOCK
Блокировки опять же в synchronized методах `HttpSession`

# Stage 4 - Шардирование
Порядок выполнения профилирования - 3 раза стреляем PUT'ами по одним и тем же ключам, после этого, без перезапуска сервера, стреляем GET'ами по тем же ключам (т.е. база не пустая).
* Было замечено, что с использованием внутреннего пула воркеров one-nio тайминги значительно больше, чем при использовании `FixedThreadPool`, в связи с чем было принято решение отказаться от пула one-nio. Так, если раньше 100% запросов при стрельбе put'ами занимали **23ms**, теперь занимают **15ms**. Get - **18ms** раньше против **8ms** сейчас.  
* На стандартном one-nio пуле воркеров при профилировании `LockSupport.park()` занимал **50%** времени CPU. При переходе на `FixedThreadPool` это время снизилось до **15%**.
* Так же, при стрельбе запросами при тех же условиях, что и раньше, таймаутов теперь нет.
## PUT
### CPU
Почти 13% времени занимает проксирование запроса к другой ноде, а вот непосредственно вставка в хранилище - всего 3%. Но ничего не поделать. 
### ALLOC
Так же, как и с CPU, немало памяти теперь выделяется при проксировании запроса (а именно при чтении с `HttpClient`'а). Да и в целом, можно заметить, что в основном память выделяется при работе с сетью, что, в общем-то, логично.
### LOCK
Как и в прошлом этапе, все локи - это методы `sendResponse` и `sendError` у `HttpSession`.
## GET
### CPU
Так же, как и в PUT - часть времени уходит на проксирование. На непосредственное взятие записи из базы - 3% (ключи в RAM).
### ALLOC
Теперь к аллокациям добавилось проксирование - и памяти для нее аллоцируется почти столько же, сколько используется при взятии значения из хранилища.
### LOCK
То же самое, что и при PUT'ах - методы `HttpSession`.

# Stage 5 - Репликация
Порядок выполнения профилирования - 3 раза стреляем PUT'ами по одним и тем же ключам с replication factor - quorum, после этого, без перезапуска сервера, стреляем GET'ами по тем же ключам (т.е. база не пустая).
* В целом, тайминги по сравнению с предыдущим этапом почти не изменились.
## PUT
### CPU
При `upsert`'е теперь часть времени уходит на отправку запроса. Проксирование больше нет, а сам метод `upsert()` занимает достаточно боьлшое количество времени, так как внутри него мы теперь ждем, пока остальные ноды добавят значение.
### ALLOC
Если раньше большое количество памяти выделялось при проксировании запроса, а именно при чтении ответа, то теперь все так же много памяти выделяется при чтении ответа с клиента, однако уже внутри самого метода `upsert()` - так как в нем мы читаем ответ с других нод.
### LOCK
В lock'ах ничего не поменялось - все те же методы `HttpSession`.
## GET
### CPU
В предыдущем этапе немалое количество времени занимали запись и чтение в сокет во время проксирования. Теперь же работа с сокетом у нас в самом методе get, соответственно, основное время тратится на него. Сама работа с хранилищем занимает все так же мало времени (относительно) - около **5%**.
### ALLOC
В целом, картина не сильно изменилась - в основном память выделяется при непосредственном обращении к хранилищу (чтобы достать `Item`), и при чтении ответа `HttpClient`а. Разница в том, что если раньше чтение ответа было при проксировании, то теперь оно находится в самом методе `get` - так как мы собираем ответы с нескольких нод.
### LOCK
Ничего нового - те же методы у сессии.

# Stage 6 - Асинхронный клиент
Порядок выполнения профилирования - 3 раза стреляем PUT'ами по одним и тем же ключам с replication factor - quorum, после этого, без перезапуска сервера, стреляем GET'ами по тем же ключам (т.е. база не пустая).
* Тайминги по сравнению с предыдущим этапом изменились незначительно, но в лучшую сторону. Ожидалось более заметное повышение производительности, вполне вероятно, что код нужно оптимизировать. Ну и погрешность измерений.
* Каллибровка у wrk2 почему то постоянно стала занимать много времени, один или два потока постоянно выбивались своими таймингами, вплоть до 13мс и даже 20мс. Возможно, связано с увеличившимся количеством используемых потоков системы.
## PUT
99 перцентиль снизился с 6-7ms до 4ms (в среднем), что очень неплохо.
### CPU
Часть задач переместилась, и теперь ими занимаются `CompletableFuture`.
### ALLOC
График очень сильно изменился, стал намного насыщеннее. по сравнению с предыдущим этапом. Теперь огромное количество аллокаций связано с выполнением `CompletableFuture`.
### LOCK
График lock'ов тоже очень сильно изменился, если раньше это были только пресловутые методы `HttpSession`, теперь тут очень много локов, связанных с асинхронными вызовами у `HttpClient`, в частности с `CompletableFuture`.
## GET
Тайминги GET'ов не сильно изменились по сравнению с предыдущим этапом (но, опять же, вполне вероятно погрешность не дает нам увидеть прирост).
### CPU
График, опять же, стал намного насыщенней. Большое количество времени теперь уходит на работу с `CompletableFuture`, через которые у нас, собственно, все и работает.
### ALLOC
Если ранее `HttpSession.processRead()` one-nio занимал 38.5% от аллокаций, то теперь 15.6% - так как немалая часть аллокаций теперь приходится на `CompletableFuture`. 
### LOCK
Картина локов сильно поменялась - большое количество блокироков, как при работе `CompletableFuture`, так и при работе `HttpClient`'а.